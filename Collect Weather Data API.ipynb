{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pyprind import ProgBar\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '9b47f685a716d06d'\n",
    "BASE_URL = 'http://api.wunderground.com/api/{}/history_{}/q/TX/Round_Rock.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"date\", \"meantempm\", \"meandewptm\", \"meanpressurem\", \"maxhumidity\",\n",
    "    \"minhumidity\", \"maxtempm\", \"mintempm\", \"maxdewptm\", \"mindewptm\",\n",
    "    \"maxpressurem\", \"minpressurem\", \"precipm\"\n",
    "]\n",
    "DailySummary = namedtuple('DailySummary', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weather_data(url, api_key, target_date, days):\n",
    "    \"\"\"Call Wunderground API to extract weather data.\"\"\"\n",
    "    records = []\n",
    "    bar = ProgBar(days)\n",
    "    for _ in range(days):\n",
    "        request = BASE_URL.format(API_KEY, target_date.strftime('%Y%m%d'))\n",
    "        response = requests.get(request)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()['history']['dailysummary'][0]\n",
    "            records.append(DailySummary(\n",
    "                date=target_date,\n",
    "                meantempm=data['meantempm'],\n",
    "                meandewptm=data['meandewptm'],\n",
    "                meanpressurem=data['meanpressurem'],\n",
    "                maxhumidity=data['maxhumidity'],\n",
    "                minhumidity=data['minhumidity'],\n",
    "                maxtempm=data['maxtempm'],\n",
    "                mintempm=data['mintempm'],\n",
    "                maxdewptm=data['maxdewptm'],\n",
    "                mindewptm=data['mindewptm'],\n",
    "                maxpressurem=data['maxpressurem'],\n",
    "                minpressurem=data['minpressurem'],\n",
    "                precipm=data['precipm']))\n",
    "        time.sleep(6)\n",
    "        bar.update()\n",
    "        target_date += timedelta(days=1)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run this cell when collecting data on day 2\n",
    "def get_target_date():\n",
    "    \"\"\"Return target date 1000 days prior to current date.\"\"\"\n",
    "    current_date = datetime.now()\n",
    "    target_date = current_date - timedelta(days=1000)\n",
    "    return target_date\n",
    "\n",
    "target_date = get_target_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:10:50\n"
     ]
    }
   ],
   "source": [
    "records = extract_weather_data(BASE_URL, API_KEY, target_date, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DailySummary(date=datetime.datetime(2016, 10, 4, 0, 0), meantempm='33', meandewptm='27', meanpressurem='1010', maxhumidity='100', minhumidity='54', maxtempm='43', mintempm='23', maxdewptm='32', mindewptm='22', maxpressurem='1011', minpressurem='1008', precipm='0.00'),\n",
       " DailySummary(date=datetime.datetime(2016, 10, 5, 0, 0), meantempm='41', meandewptm='32', meanpressurem='1012', maxhumidity='100', minhumidity='62', maxtempm='50', mintempm='33', maxdewptm='32', mindewptm='32', maxpressurem='1014', minpressurem='1011', precipm='0.00'),\n",
       " DailySummary(date=datetime.datetime(2016, 10, 6, 0, 0), meantempm='38', meandewptm='30', meanpressurem='1013', maxhumidity='100', minhumidity='57', maxtempm='48', mintempm='28', maxdewptm='32', mindewptm='28', maxpressurem='1015', minpressurem='1011', precipm='0.00'),\n",
       " DailySummary(date=datetime.datetime(2016, 10, 7, 0, 0), meantempm='29', meandewptm='24', meanpressurem='1017', maxhumidity='100', minhumidity='55', maxtempm='42', mintempm='17', maxdewptm='31', mindewptm='14', maxpressurem='1022', minpressurem='1014', precipm='0.00'),\n",
       " DailySummary(date=datetime.datetime(2016, 10, 8, 0, 0), meantempm='28', meandewptm='19', meanpressurem='1023', maxhumidity='94', minhumidity='49', maxtempm='42', mintempm='15', maxdewptm='32', mindewptm='14', maxpressurem='1025', minpressurem='1021', precipm='0.00')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first five records\n",
    "records[400:405]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DailySummary(date=datetime.datetime(2016, 10, 3, 0, 0), meantempm='24', meandewptm='18', meanpressurem='1013', maxhumidity='94', minhumidity='41', maxtempm='33', mintempm='17', maxdewptm='22', mindewptm='15', maxpressurem='1016', minpressurem='1011', precipm='0.00')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect last record to date; next target date should be plus one day\n",
    "records[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new target date based on date above plus one day\n",
    "target_date = datetime(2016, 10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:14\n"
     ]
    }
   ],
   "source": [
    "# run this 4 separate times after setting the target date\n",
    "records += extract_weather_data(BASE_URL, API_KEY, target_date, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save records list\n",
    "with open('records_pt1.pkl', 'wb') as f:\n",
    "    pickle.dump(records, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load records list - still need to run cells 1-4\n",
    "with open('records_pt1.pkl', 'rb') as fp:\n",
    "    records = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect last record to date; next target date should be plus one day\n",
    "records[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new target date based on date above plus one day\n",
    "target_date = datetime(2018, 2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this 5 separate times after setting the target date\n",
    "records += extract_weather_data(BASE_URL, API_KEY, target_date, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('records_pt2.pkl', 'wb') as f:\n",
    "    pickle.dump(records, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load records list - still need to run cells 1 and 3\n",
    "with open('records_pt2.pkl', 'rb') as fp:\n",
    "    records = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records, columns=features).set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[['meantempm', 'meandewptm']].head(10)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 day prior\n",
    "N = 1\n",
    "\n",
    "# target measurement of mean temperature\n",
    "feature = 'meantempm'\n",
    "\n",
    "# total number of rows\n",
    "rows = tmp.shape[0]\n",
    "\n",
    "# a list representing Nth prior measurements of feature\n",
    "nth_prior_measurements = tmp[feature].shift(periods=N)\n",
    "\n",
    "# makee a new column name of feature_N and add to DataFrame\n",
    "col_name = f'{feature}_{N}'\n",
    "tmp[col_name] = nth_prior_measurements\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_nth_day_feature(df, feature, N):\n",
    "    rows = df.shape[0]\n",
    "    nth_prior_measurements = df[feature].shift(periods=N)\n",
    "    col_name = f'{feature}_{N}'\n",
    "    df[col_name] = nth_prior_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    if feature != 'date':\n",
    "        for N in range(1, 4):\n",
    "            derive_nth_day_feature(df, feature, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of original features without meantempm, mintempm, and maxtempm\n",
    "to_remove = [feature\n",
    "             for feature in features\n",
    "             if feature not in ['meantempm', 'mintempm', 'maxtempm']]\n",
    "\n",
    "# make a list of columns to keep\n",
    "to_keep = [col for col in df.columns if col not in to_remove]\n",
    "\n",
    "# select only the columns in to_keep and assign to df\n",
    "df = df[to_keep]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call describe on df and transpose it due to the large number of columns\n",
    "spread = df.describe().T\n",
    "\n",
    "# precalculate interquartile range for ease of use in next calculation\n",
    "IQR = spread['75%'] - spread['25%']\n",
    "\n",
    "# create an outliers column which is either 3 IQRs below the first quartile or\n",
    "# 3 IQRs above the third quartile\n",
    "spread['outliers'] = (spread['min'] <\n",
    "                      (spread['25%'] -\n",
    "                       (3 * IQR))) | (spread['max'] >\n",
    "                                      (spread['75%'] + 3 * IQR))\n",
    "\n",
    "# just display the features containing extreame outliers\n",
    "spread.loc[spread.outliers, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "ax.hist(df.maxhumidity_1)\n",
    "ax.set_title('Distribution of maxhumidity_1')\n",
    "ax.set_xlabel('maxhumidity_1')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "ax.hist(df.minpressurem_1)\n",
    "ax.set_title('Distribution of minpressurem_1')\n",
    "ax.set_xlabel('minpressurem_1')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the precip columns\n",
    "for precip_col in ['precipm_1', 'precipm_2', 'precipm_3']:\n",
    "    # create a boolean array of values representing nans\n",
    "    missing_vals = pd.isnull(df[precip_col])\n",
    "    df[precip_col][missing_vals] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "with open('end-part1_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
